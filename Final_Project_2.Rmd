---
title: "Data 621 Final Project"
author: "(Group 4) Eddie Xu, Mohamed Hassan-El Serafi, Chun Shing Leung, Keith Colella, Yina,
  Qiao"
date: "2024-11-16"
output: html_document
---

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(mlogit)
library(ggplot2)
library(cowplot)
library(caret)
library(MASS)
library(randomForest)
library(car)
library(pROC)
library(reshape2)
library(visdat)
```

The data set is from Kaggle:
https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset


```{r}
mydata<- read.csv("https://raw.githubusercontent.com/tonyCUNY/DATA_621/refs/heads/main/heart.csv")
```

## EDA - Exploratory Data Analysis

Attribute Information:
age
sex
chest pain type (4 values)
resting blood pressure
serum cholestoral in mg/dl
fasting blood sugar > 120 mg/dl
resting electrocardiographic results (values 0,1,2)
maximum heart rate achieved
exercise induced angina
oldpeak = ST depression induced by exercise relative to rest
the slope of the peak exercise ST segment
number of major vessels (0-3) colored by flourosopy
thal: 0 = normal; 1 = fixed defect; 2 = reversable defect

```{r}
# Descriptive statistics
str(mydata)
```

```{r}
#Transform the variables as factor and numeric
mydata.clean <- mydata %>%
  mutate(
    sex = factor(ifelse(sex == 0, "F", "M")),
    age = as.numeric(age),
    trestbps = as.numeric(trestbps),
    chol = as.numeric(chol),
    thalach = as.numeric(thalach),
    cp = as.factor(cp),
    fbs = as.factor(fbs),
    restecg = as.factor(restecg),
    exang = as.factor(exang),
    slope = as.factor(slope),
    ca = as.factor(as.integer(ca)),
    thal = as.factor(as.integer(thal)),
    target = factor(ifelse(target == 0, "No_Disease", "Disease"))
  )

```

```{r}
str(mydata.clean)
```
```{r}
# Statistical summary
summary(mydata.clean)
```
## Checking for Missing Value

1. No Missing Value for both numeric and Categorical Variables

```{r, echo=FALSE}
num_vars <- mydata.clean %>% select_if(where(is.numeric))
vis_miss(num_vars, cluster = TRUE) + 
  ggtitle("Numeric Variables \n- Most Missing Values (INCOME, HOME_VAL, AGE, YOJ, CAR_AGE)") +
  theme(
    plot.title = element_text(face = "bold"),
    plot.margin = unit(c(1, 2, 1, 1), "cm")
  )
```

```{r, echo=FALSE}
cat_vars <- mydata.clean %>% select_if(~ is.factor(.))
cat_vars <- cat_vars %>% 
  mutate(across(everything(), ~na_if(as.character(.), "")))
vis_miss(cat_vars, cluster = TRUE) +
  ggtitle("Categorical Variables \n- Most Missing Values (JOB)") +
  theme(
    plot.title = element_text(face = "bold"),
    plot.margin = unit(c(1, 2, 1, 1), "cm")
  )
```

## Box plot

```{r, warning=FALSE}

cat_vars%>%
  gather() %>%
  ggplot(aes(value)) +
  geom_bar(fill = "lightblue", color="grey") +
  facet_wrap(~ key, scales = "free", ncol = 4) +
  theme(
    panel.grid = element_blank(), 
    axis.text.x = element_text(angle = 45, hjust = 1)
  ) +
  labs(title = "Bar Plots of Categorical Variables")

```


## Correlation Matrix

```{r vif_model, warning=FALSE, message=FALSE}
cor_matrix <- cor(mydata.clean %>% select_if(where(is.numeric)), use = "complete.obs")

cor_long <- melt(cor_matrix)
cor_long <- cor_long[as.numeric(cor_long$Var1) > as.numeric(cor_long$Var2), ]


ggplot(cor_long, aes(Var2, Var1, fill = value)) +
  geom_tile(color = "white") +
  geom_text(aes(label = ifelse(value != 0, round(value, 2), "")), 
            color = "black", size = 3, face="bold") +  # Show only significant values
  scale_fill_gradient2(low = "pink", high = "blue", mid = "white", 
                       midpoint = 0, limit = c(-1, 1), 
                       space = "Lab", name = "Correlation") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1, size = 10),  # Adjust x-axis label
    axis.text.y = element_text(size = 10),                                   # Adjust y-axis label
    axis.title = element_blank(),                                            # Remove axis titles
    plot.title = element_text(hjust = 0.5, face = "bold", size = 14)          # Center plot title
  ) +
  ggtitle("Correlation Matrix")
```

##  VIF Check

the VIF score analysis is conducted to check for any multicollinearity.

```{r}
# fit a linear regression before VIF score
vif_model_all <- lm(as.numeric(target) ~ ., data = mydata.clean)

summary(vif_model_all)
```

After the model fitting, 

The following variables has P-value larger than 0.05, showing that predictor variables may not be significantly associated with the outcome

age
fbs1        
slope1
restecg2

```{r}
# perform VIF
vif_value = vif(vif_model_all)
vif_value
```



## Data Balance 
1. Target is fairly balanced (Almost 50-50% with no disease/disease)
2. Each level of categorical variables are represented by lots of patients
3. We can conclude the data is not imbalanced

```{r}

summary_data <- mydata.clean %>%
  group_by(target) %>%
  summarise(sum_count = n()) %>%
  mutate(percentage = (sum_count / sum(sum_count)) * 100)


ggplot(summary_data, aes(x = target, y = sum_count, fill = target)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = paste0(round(percentage, 1), "%")), 
            position = position_stack(vjust = 0.5), size = 5, color = "white") + 
  labs(title = "Sum Count of Target with Percentages", 
       x = "Target Value", 
       y = "Sum Count") +
  theme_minimal()
```


```{r}
# 
variables <- c("sex", "cp", "fbs", "restecg", "slope", "ca")
plot_data <- lapply(variables, function(var) {
  mydata.clean |> 
    group_by(across(all_of(var)), target) |> 
    summarise(count = n(), .groups = "drop") |> 
    mutate(variable = var, level = get(var))
}) %>%
  bind_rows()

# Create the faceted bar plot
ggplot(plot_data, aes(x = level, y = count, fill = factor(target))) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~variable, scales = "free_x") +
  labs(
    title = "Relationship between Target and Other Variables",
    x = "Variable Levels",
    y = "Count",
    fill = "Target"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

## Model 1 -  Logistic Regression

```{r}

logistic_full <- glm(target ~., data=mydata.clean, family = "binomial")
summary(logistic_full)

```
## Improve the model with StepAIC

1. fbs, restecg are removed.

```{r}

logistic_2 <- stepAIC(logistic_full)
summary(logistic_2)

```

1. We use K-fold cross validation 

"In K-fold cross-validation, the data set is divided into a number of K-folds and used to assess the model's ability as new data become available. K represents the number of groups into which the data sample is divided. For example, if you find the k value to be 5, you can call it 5-fold cross-validation."

2. Here we use 10 fold
3. Around 922 samples are used in training set and remaining 103 samples are used as test set. 
4. Around 87% accuracy


```{r}
# Assess model using Cross Validation

crossValSettings <- trainControl(method = "repeatedcv", number = 10, savePredictions = TRUE)

crossVal <- train(target ~ age + sex + cp + trestbps + chol + thalach + 
    exang + oldpeak + slope + ca + thal, data=mydata.clean, family = "binomial", method = "glm", trControl = crossValSettings)

crossVal

pred <- predict(crossVal, newdata = mydata.clean)
confusionMatrix(data = pred, mydata.clean$target)

```

## Model 2 - Random Forest 

```{r}
# Split the data
set.seed(123)
indexSet <-sample(2, nrow(mydata.clean), replace = T, prob = c(0.8, 0.2))
train <- mydata.clean[indexSet==1,]
test <- mydata.clean[indexSet==2,]

```

1. A very low OOB (Out-of-bag) value suggest overfitting:
2. Reducing the complexity of the random forest model by decreasing the number of trees, limiting the maximum depth of trees, or removing features.

```{r}
set.seed(1234)
rf <- randomForest(target ~ ., data=train, proximity=TRUE, importance =TRUE, do.trace = 10, ntree = 50)
rf
```
1. Error rate become relatively flat after 10 trees

```{r}
# Plot the error rate to see what is the suitable number of trees.

oob.error.data <- data.frame(
  Trees=rep(1:nrow(rf$err.rate), times=3),
  Type=rep(c("OOB", "Disease", "No_Disease"), each=nrow(rf$err.rate)),
  Error=c(rf$err.rate[,"OOB"], 
    rf$err.rate[,"Disease"], 
    rf$err.rate[,"No_Disease"]))

ggplot(data=oob.error.data, aes(x=Trees, y=Error)) +
  geom_line(aes(color=Type))

```

1. Here we select 10 as number of tree
2. OOB is around 5% means 95% samples were correctly classified by the random forest

```{r}
set.seed(1235)
rf2 <- randomForest(target ~ ., data=train, proximity=TRUE, importance =TRUE, ntree = 5)
rf2
```
```{r}
#Variable Importance - which variables can we remove?

varImpPlot(rf2)
```

1. fbs, restecg are removed as they are less important 

```{r}
set.seed(12)
rf3 <- randomForest(target ~ age + sex + cp + trestbps + chol + thalach + 
    exang + oldpeak + slope + ca + thal, data=train, proximity=TRUE, importance =TRUE, ntree = 5)
rf3

```

```{r}
result <- data.frame(test$target, predict(rf3, test[, 0:13], type="response"))
head(result)
```
```{r}
confusionMatrix(result$predict.rf3..test...0.13...type....response.., result$test.target)
```


## Select Mode -ROC Curve and AUC

Random Forest Model has higher AUC and below ROC Curve suggest Random Forest Model is better.

```{r}
par(pty="s")
roc(mydata$target, logistic_2$fitted.values, plot=TRUE, legacy.axes=TRUE, percent=TRUE,
    xlab="False Positive Percentage", ylab="True Positive Percentage", col="#0ABAB5",  print.auc=TRUE)
plot.roc(train$target, rf3$votes[,1], percent=TRUE, col="#FA8072", print.auc=TRUE, add=TRUE, print.auc.y=40)
legend("bottomright", legend=c("Logistic Regression", "Random Forest"), col=c("#0ABAB5","#FA8072"), lwd=2)
```

